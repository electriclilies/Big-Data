{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this document, I create a neural network, and explore methods of training the network.\n",
    "A neural network consists of layers of nodes. Each layer of nodes passes output to each node in the next layer. We call the connections between nodes in different layers \"edges\". The number of edges connected to each node is equal to the sum of the number of edges in the two adjacent layers. These edges are each assigned different weights. To get the output of each node, the inputs are multiplied by the weight associated with it, and then sumed. The sum is then passed through a function. Usually this is refered to as an activation function. I used the hyperbolic tangent function. \n",
    "After the inputs have passed all the way through the network, the final output is compared to the known output. Then, the accuracy of the network is determined. Finally, a program adjusts the weights to optimize the accuracy (or minimize the error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Neural Network\n",
    "Below are two functions that I use to create a randomly generated neural network. The first function, createLayer, creates a layer of nodes. Each node is a list of edges (randomly generated weights between -1 and 1) connecting it to the nodes in the previous layer. The second function, createNetwork, creates a list of layers. The final structure is a list of lists of floats. \n",
    "Although createNetwork creates a network with layers that contain the same number of nodes, the createLayer function could be used to create networks with layers containing different numbers of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "#creates a layer of random node weights\n",
    "def createLayer(node_number, input_number):\n",
    "    layer = []\n",
    "    #creates each node\n",
    "    for i in range(node_number):\n",
    "        node = []\n",
    "        #creates weights for each node (number of weights = number of inputs node gets = number of nodes in prev. layer)\n",
    "        for j in range(input_number): \n",
    "            node.append((0.5-random.random())*2)\n",
    "        layer.append(node)\n",
    "    return layer\n",
    "    \n",
    "#creates a network of arrays\n",
    "def createNetwork(hid_layer_number, init_input_number, layer_size):\n",
    "    layers = []\n",
    "    #creates an input layer \n",
    "    layers.append(createLayer(layer_size, init_input_number))\n",
    "    for i in range(hid_layer_number):\n",
    "        layers.append(createLayer(layer_size, layer_size))\n",
    "    layers.append(createLayer(1, layer_size))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Output of the Network\n",
    "Each node is passed a value from every other node in the previous layer (the first layer is passed the input). To compute each node's outputs, you multiply each input by its weight, and sum them. This is then passed through an activation function. In this case, I used the hyperbolic tangent function, but you can use any bounded function. \n",
    "The function createOutput computes this value. \n",
    "The next function, runNetwork, creates the final output of the network. For the first layer, runNetwork passes in the input data, and gets a list of outputs from each node in that layer. It then runs createOutput for the next layer, this time with the outputs of the previous layer as inputs. The final layer has a single node, and produces a single output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createOutput(input_list, weight_list):\n",
    "    weighted_sum = 0\n",
    "    #takes the dot product of the input list and weight list, then feeds it through the hyperbolic tangent function\n",
    "    for i in range(len(input_list)):\n",
    "        weighted_sum+=(weight_list[i]*input_list[i])\n",
    "    #takes the hyperbolic tangent of the weighted sum\n",
    "    return math.tanh(weighted_sum)\n",
    "\n",
    "def runNetwork(Input, network):\n",
    "    #iterates through layers\n",
    "    for i in range(len(network)): \n",
    "        #iterates through nodes in layers\n",
    "        output_list = []\n",
    "        #creates the outputs of each layer, then feeds them into the next layer\n",
    "        for j in range(len(network[i])):\n",
    "            output_list.append(createOutput(Input, network[i][j]))\n",
    "        Input = output_list\n",
    "    return output_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Project\n",
    "When I started the project, I used some simple data. I wanted my network to take 2 inputs, x and y, and compute whether x was greater than y. To do this, I created a dataset that had an output of -1 if x < y, and an output of 1 if x > y. \n",
    "I had my neural network round the final output so it would give me a binary result. Then, I could feed all the data through the network and compute the percent of results that were accurate, and try to maximize that. \n",
    "I tried a few methods to maximize accuracy. First, I simply replaced each weight with a new, randomly generated weight. If the accuracy of the network was greater than before, I kept the weight. If not, I threw it out and tried again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runNetworkBinary(Input, nework):\n",
    "        #iterates through layers\n",
    "    for i in range(len(network)): \n",
    "        #iterates through nodes in layers\n",
    "        output_list = []\n",
    "        #creates the outputs of each layer, then feeds them into the next layer\n",
    "        for j in range(len(network[i])):\n",
    "            output_list.append(createOutput(Input, network[i][j]))\n",
    "        Input = output_list\n",
    "        #ROUNDS the output to get a binary output\n",
    "    return round(output_list[0])\n",
    "\n",
    "def createDataBinary(length): \n",
    "    data_list = []\n",
    "    output_list = []\n",
    "    for i in range(length):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x/y) > 1: \n",
    "            above_line = 1\n",
    "        elif (x/y) == 1: \n",
    "            above_line = 0\n",
    "        elif (x/y) < 1: \n",
    "            above_line = -1\n",
    "            \n",
    "        data_list.append([x, y])\n",
    "        output_list.append(above_line)\n",
    "    return data_list, output_list\n",
    "\n",
    "def getAccuracy(input_list, output_list, network):\n",
    "    accurate_total = 0\n",
    "    predicted_output = []\n",
    "    for i in range(len(input_list)):\n",
    "        output = runNetworkBinary(input_list[i], network)\n",
    "        predicted_output.append(output)\n",
    "        if output == output_list[i]:\n",
    "            accurate_total+=1\n",
    "    accuracy = accurate_total/len(output_list) * 100\n",
    "    return accuracy, predicted_output\n",
    "\n",
    "    \n",
    "#randomly adjusts the weights until it finds a better one  \n",
    "def trainRandom(input_list, output_list, network):\n",
    "    oldAccuracy = getAccuracy(input_list, output_list, network)[0]\n",
    "    print(\"Initial Accuracy:\", oldAccuracy, \"%\")\n",
    "    for i in range(len(network)):\n",
    "        for j in range(len(network[i])):\n",
    "            for k in range(len(network[i][j])):\n",
    "                #stores the old weight (copy function important, if not there python just makes a pointer)\n",
    "                old_weight = copy.copy(network[i][j][k])\n",
    "                #picks a new random value for network[i][j][k]\n",
    "                network[i][j][k] = 2*(random.random()-0.5) #assigns a new random value to the edge\n",
    "                newAccuracy = getAccuracy(input_list, output_list, network)[0] \n",
    "                tries=0\n",
    "                while newAccuracy <= oldAccuracy:\n",
    "                    tries+=1\n",
    "                    network[i][j][k] = 2*(random.random()-0.5)\n",
    "                    newAccuracy = getAccuracy(input_list, output_list, network)[0]\n",
    "                    if tries>250:\n",
    "                        tries=0\n",
    "                        network[i][j][k] = old_weight\n",
    "                        print(\"no better weight found\")\n",
    "                        break\n",
    "                tries=0\n",
    "                if newAccuracy > oldAccuracy:\n",
    "                    oldAccuracy = newAccuracy\n",
    "                    print(oldAccuracy)\n",
    "    return network, oldAccuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I run the function that randomly changes each weight until it finds a better one. If it has tried 250 times, the function moves on to the next weight. Interestingly, if the neural network is particularly small, sometimes the function doesn't get past 0.0% accuracy. Other times, it moves past 0.0% fairly soon--and then improves quickly. This also depends on the size of the network--the larger it is, the more accurate (or closer to random) it is in the beginning. (You should run the box below a few times to see for yourself). \n",
    "This system didn't work very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy: 47.0 %\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[[-0.9848969856379275, -0.4288850071224177],\n",
       "   [-0.31590984652007115, -0.8431560176025068],\n",
       "   [-0.45636020210272776, -0.8023920065529486],\n",
       "   [0.7015539133813553, 0.9238309808234924],\n",
       "   [-0.20830101968296133, -0.8996568231111568]],\n",
       "  [[0.6015437212329693,\n",
       "    0.6723850132964759,\n",
       "    0.38968648703624376,\n",
       "    -0.9351444817056864,\n",
       "    0.6198161392084254],\n",
       "   [-0.3760584101711655,\n",
       "    0.08510160093227381,\n",
       "    -0.22266831273422638,\n",
       "    0.5343761437248851,\n",
       "    -0.9224251725869177],\n",
       "   [-0.9961571383554302,\n",
       "    0.1062217094511202,\n",
       "    -0.48440131146000565,\n",
       "    -0.9111369978264869,\n",
       "    -0.6063941839122344],\n",
       "   [-0.06397099787648175,\n",
       "    -0.5673572953642756,\n",
       "    0.27067494493054434,\n",
       "    -0.5713073498018966,\n",
       "    0.7615428214008892],\n",
       "   [0.17876598067810834,\n",
       "    0.5807275017104541,\n",
       "    -0.44354361870954384,\n",
       "    -0.287176341114068,\n",
       "    0.3986534287446135]],\n",
       "  [[-0.6182693883947992,\n",
       "    -0.6037925503498205,\n",
       "    0.5009123786558063,\n",
       "    -0.22336929417896156,\n",
       "    0.24855874578678194],\n",
       "   [-0.9792885605411601,\n",
       "    -0.1651758996410595,\n",
       "    -0.7284345350893726,\n",
       "    -0.023870317322783663,\n",
       "    -0.9275755965625432],\n",
       "   [-0.20727899570679242,\n",
       "    0.7991167145802487,\n",
       "    -0.087457462362456,\n",
       "    -0.9835157188323327,\n",
       "    -0.33377266421120755],\n",
       "   [0.718205780977859,\n",
       "    0.05489046760621741,\n",
       "    -0.6981240626573044,\n",
       "    -0.6703132706942618,\n",
       "    0.7591401748379609],\n",
       "   [0.24538618050416527,\n",
       "    -0.34203050270291313,\n",
       "    0.26363990216618616,\n",
       "    0.42608158833849585,\n",
       "    -0.2800769640820908]],\n",
       "  [[0.9837720243774191,\n",
       "    -0.6112112184286627,\n",
       "    0.2081229176103363,\n",
       "    0.9112920044100974,\n",
       "    0.870659058939323],\n",
       "   [-0.7004930085422343,\n",
       "    -0.6271135909894052,\n",
       "    -0.672608088616653,\n",
       "    0.6819684546066882,\n",
       "    -0.8578693336000511],\n",
       "   [-0.0147539988682992,\n",
       "    -0.8164834394916083,\n",
       "    -0.4469088811717108,\n",
       "    -0.005877620639481496,\n",
       "    0.7716067944792153],\n",
       "   [0.14279350624708376,\n",
       "    0.8753648980017799,\n",
       "    0.6688026649611849,\n",
       "    -0.6612092381427035,\n",
       "    -0.7276857736434232],\n",
       "   [0.8420891258666627,\n",
       "    0.8737582112697622,\n",
       "    0.6165119765922069,\n",
       "    -0.9366233745387196,\n",
       "    -0.007838543338952064]],\n",
       "  [[0.04263797064651742,\n",
       "    0.9233075832330695,\n",
       "    0.38828739293239223,\n",
       "    -0.6874163981311523,\n",
       "    -0.21433682580816882]]],\n",
       " 47.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list, output_list = createDataBinary(100)\n",
    "Network = createNetwork(3, 2, 5)\n",
    "trainRandom(input_list, output_list, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I decided to find a network that was doing OK initially, and then change each weight by a little bit until the function was more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOKNetwork(input_list, output_list):\n",
    "    network = createNetwork(1, 2, 2) #three hidden layers, 2 inputs, 5 nodes/layer\n",
    "    accuracy = getAccuracy(input_list, output_list, network)[0]\n",
    "    print(accuracy)\n",
    "    for i in range(200):\n",
    "        new_network = createNetwork(1, 2, 4)\n",
    "        new_accuracy = getAccuracy(input_list, output_list, new_network)[0]\n",
    "        if new_accuracy > accuracy:\n",
    "            print(\"found better network\")\n",
    "            network = new_network\n",
    "            accuracy = new_accuracy\n",
    "    print(\"Best Accuracy:\", accuracy)\n",
    "    return network, accuracy\n",
    "\n",
    "\n",
    "def trainStep(input_list, output_list, network, scale_init):\n",
    "    scale = 1\n",
    "    for x in range(3):\n",
    "        #makes the scale smaller and smaller with every repeat\n",
    "        scale *= scale_init\n",
    "        for y in range(4):\n",
    "            oldAccuracy = getAccuracy(input_list, output_list, network)[0]\n",
    "            print(\"Initial Accuracy:\", oldAccuracy, \"%\")\n",
    "            for i in range(len(network)):\n",
    "                for j in range(len(network[i])):\n",
    "                    for k in range(len(network[i][j])):\n",
    "                        #copies the old weight\n",
    "                        old_weight = copy.copy(network[i][j][k])\n",
    "                        #adjusts the weight by a little bit\n",
    "                        network[i][j][k] += scale * (0.5-random.random())*2.0\n",
    "                        #gets the new accuracy\n",
    "                        newAccuracy = getAccuracy(input_list, output_list, network)[0]\n",
    "                        tries=0\n",
    "                        while newAccuracy <= oldAccuracy:\n",
    "                            tries+=1\n",
    "                            network[i][j][k] = old_weight\n",
    "                            network[i][j][k] += scale * (0.5-random.random())*2.0\n",
    "                            newAccuracy = getAccuracy(input_list, output_list, network)[0]\n",
    "                            if tries>250:\n",
    "                                network[i][j][k] = old_weight\n",
    "                                print(\"no better weight found\")\n",
    "                                break\n",
    "                        tries=0\n",
    "                        if newAccuracy > oldAccuracy: \n",
    "                            oldAccuracy = newAccuracy\n",
    "                            print(oldAccuracy)\n",
    "    return network, oldAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, I get some OK networks. Jupyter is being very weird: if I run this in spyder, I get different accuracies and neworks every time I run the function. This is true for trainRandom as well. Here, you only get a new one if you reinitialize the kernel. I have no clue why, but I think it has something to do with the way jupyter stores variables, or produces random numbers..\n",
    "The spyder files that are also in the github work, though they're not formatted nicely. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy: 47.0 %\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "no better weight found\n",
      "47.0\n",
      "Best Accuracy: 47.0\n",
      "47.0\n",
      "Best Accuracy: 47.0\n"
     ]
    }
   ],
   "source": [
    "Network2, acc = getOKNetwork(input_list, output_list)\n",
    "Network3, acc = getOKNetwork(input_list, output_list)\n",
    "#print(\"Start Step Training\")\n",
    "#trainStep(input_list, output_list, Network, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documenting how the accuracy changes when weights are changed\n",
    "This function cycles through a network. It adds a little bit to a weight, records the new accuracy of the network in a another array of the same structure, then reinitializes the network, and does it again for the next weight. \n",
    "I had A LOT of trouble with this. It kept giving me different accuracies when I called the function multiple times with the same input. \n",
    "I spent probably four hours trying to debug the program. Finally, I figured out that the problem was that when I said networkCopy = network, I wasn't actually making a copy of the network, I was just making a pointer. I fixed it by saying networkCopy = copy.deepcopy(network). \n",
    "The function returns an array of the change in accuracy in the same structure as the original network, and an array of the derivative of accuracy (change in accuracy divided by step size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accArray(input_list, output_list, network, step):\n",
    "    initial_acc = getAccuracy(Input, Output, network)[0]\n",
    "    print(\"Initial Accuracy:\", initial_acc)\n",
    "    accArray = copy.deepcopy(network)\n",
    "    derivArray = copy.deepcopy(network)\n",
    "    for i in range(len(network)):\n",
    "        for j in range(len(network[i])):\n",
    "            for k in range(len(network[i][j])):\n",
    "                #initializes networkCopy to be the same as network\n",
    "                networkCopy = copy.deepcopy(network)\n",
    "                networkCopy[i][j][k] += step\n",
    "                accArray[i][j][k] = initial_acc - getAccuracy(input_list, output_list, networkCopy)[0]\n",
    "                derivArray[i][j][k]= accArray[i][j][k]/step\n",
    "    return accArray, derivArray\n",
    "\n",
    "print(accArray(input_list, output_list, Network2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am switching back to spyder because the problem discussed above with generating networks is annoying me, and I do not want to fix it. Please look in the backpropogation.py file (not all the code above is in that file, only new stuff). More writing is below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some writing about what is in that other file:\n",
    "-I started looking more closely into backpropogation, then decided not to do it. However, I decided to change from calculating accuracy to calculating a squared error value. I also stopped rounding my outputs. So, even though the acutal outputs of my data set are binary, I have make floats that are close to the binary numbers. \n",
    "-The function getSquaredError gets the average squared error of the data set\n",
    "-The function errArray creates an array of the difference between squared errors after changing a weight (it functions exactly like accArray but with squared errors instead)\n",
    "-The function scaleWeight (last function in the file) takes each weight adds the step size that was added in errArray times the change in error. It does that until the error gets worse, or it has done it five times. Then, it moves onto the next weight. \n",
    "-The code at the very bottom creates a dataset. Then, it creates a randomly generated neural network that is pretty good (using the getOKNetwork function). Next, it trains the network using the scaleError function 100 times, printing the new accuracy each time. Finally, the program creates a new dataset, which is then fed through the improved network. The program then prints the actual and predicted value of the new dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
